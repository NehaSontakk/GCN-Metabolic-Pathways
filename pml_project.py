# -*- coding: utf-8 -*-
"""PML Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TfXANmEGZYhZhjqb7cHLhG_ECL6kuK3i
"""

import os
import glob
import json
import pandas as pd
import numpy as np
from math import log, exp

module_id      = "M00001"
sample_name    = "3300060297_1"

# I have uploaded 10 graph examples to KEGG_Graphs_Generated

COMPLETENESS_CSV       = "Data/Contig_completeness.csv"
VK_CSV                 = "Data/KO_trusted_cutoff.csv"
KO_OCCURRENCES_TXT     = "Data/ko_occurences.txt"
ADJ_LINKS_JSON         = "Data/all_module_adjacency_links_Apr3.json"
neighbor_txt = "Data/ko_normalized_prediction.txt"
OUTPUT_EXCEL           = f"{module_id}_KO_table.xlsx"

MODULE_JSON_DIR = "KEGG_Graphs_Generated/"
nodes_path = f"{MODULE_JSON_DIR}/module_{module_id}_nodes.json"
with open(nodes_path) as f:
    module_nodes = json.load(f)

# Extract just the KO IDs in this module (base 6‐char codes)
module_kos = sorted({n.split("_",1)[0] for n in module_nodes if n.startswith("K")})

"""## Node Probability: Preparing for dk values for each KO"""

# completeness
df_comp = pd.read_csv(COMPLETENESS_CSV)
df_comp['sigma'] = 1 - ((np.exp(3 * df_comp['Completeness']) - 1) / np.exp(3))

# VK table
df_vk = pd.read_csv(VK_CSV)
df_vk['E_values_MAG']    = df_vk['E_values_MAG'].fillna(1e4)
df_vk['E_value_trusted'] = df_vk['E_value_trusted'].fillna(1e-5)
df_vk['Rk']   = df_vk['E_values_MAG'] / df_vk['E_value_trusted']
df_vk['Vk']   = df_vk['Rk'].apply(lambda x: max(0, -np.log10(x)))
df_vk['alphak'] = df_vk['Vk'].apply(lambda x: 1 - np.exp(-x / 5))
df_vk['q_half'] = df_vk['E_value_trusted'] * (10 ** -3.46574)

# merge in KO frequencies
ko_occ = pd.read_csv(KO_OCCURRENCES_TXT, sep="\s+",names=['KO id','occurences'])
ko_occ['KO_freq'] = ko_occ['occurences'] / 895

df_vk_prob = pd.merge(df_vk, ko_occ, on="KO id", how="left")
df_vk_prob['KO_freq'] = df_vk_prob['KO_freq'].fillna(1e-3)

# pick sigma for your sample
sigma_val = df_comp.loc[df_comp['filename']==sample_name, 'sigma'].iloc[0]

"""### Gamma KO FUNCTION & D KO"""

def gamma_func(row, t=np.float128(1e-5)):
    q = np.float128(row['E_value_trusted'])
    mag = np.float128(row['E_values_MAG'])
    qh = q * (np.float128(10) ** np.float128(-3.46574))
    alpha = np.float128(row['alphak'])
    if mag > t:
        return np.float128(0)
    elif mag > qh:
        return (np.log10(mag/t) / np.log10(qh/t)) * np.float128(0.5)
    else:
        return alpha

df_vk_prob['gamma_k'] = df_vk_prob.apply(gamma_func, axis=1)
df_vk_prob['Dk'] = df_vk_prob.apply(
    lambda r: r['gamma_k'] + (1 - r['gamma_k']) * sigma_val * r['KO_freq'],
    axis=1
)
df_vk_prob['sigma'] = sigma_val

nodes_path = os.path.join(MODULE_JSON_DIR, f"module_{module_id}_nodes.json")
with open(nodes_path) as f:
    module_nodes = json.load(f)

#  lookup
dk_dict = dict(zip(df_vk_prob['KO id'], df_vk_prob['Dk']))

module_nodes_with_DK = {}
for node, idx in module_nodes.items():
    if node.startswith("K"):
        base = node.split("_",1)[0]
        module_nodes_with_DK[node] = {
            "index": idx,
            "Dk": dk_dict.get(base, np.nan)
        }
    else:
        module_nodes_with_DK[node] = {"index": idx}

ko_set = {n.split("_",1)[0] for n in module_nodes if n.startswith("K")}
print(ko_set)
missing_kos = ko_set.difference(df_vk_prob['KO id'])

if missing_kos:
    # constant pieces we’ll reuse
    _defaults = {
        "E_value_trusted": 1e-5,
        "E_values_MAG":    1e4,
        "KO_freq":         1e-3,
        "sigma":           sigma_val        # use the same sigma for this sample
    }

    new_rows = []
    for ko in missing_kos:
        # recompute the derived columns exactly the same way
        Rk       = _defaults["E_values_MAG"] / _defaults["E_value_trusted"]
        Vk       = max(0, -np.log10(Rk))
        alphak   = 1 - np.exp(-Vk / 5)
        q_half   = _defaults["E_value_trusted"] * (10 ** -3.46574)

        # γ_k with the same rules as `gamma_func`
        t        = np.float128(1e-5)
        mag128   = np.float128(_defaults["E_values_MAG"])
        qh128    = np.float128(q_half)
        if mag128 > t:
            gamma_k = 0.0                     # falls into the first branch of gamma_func
        elif mag128 > qh128:
            gamma_k = (np.log10(mag128/t) / np.log10(qh128/t)) * 0.5
        else:
            gamma_k = alphak

        # finally D_k
        Dk = gamma_k + (1 - gamma_k) * sigma_val * _defaults["KO_freq"]

        new_rows.append({
            "KO id": ko,
            "E_value_trusted": _defaults["E_value_trusted"],
            "E_values_MAG":    _defaults["E_values_MAG"],
            "Rk":              Rk,
            "Vk":              Vk,
            "alphak":          alphak,
            "q_half":          q_half,
            "gamma_k":         gamma_k,
            "KO_freq":         _defaults["KO_freq"],
            "sigma":           sigma_val,
            "Dk":              Dk
        })

    # append to the main probability table
    df_vk_prob = pd.concat([df_vk_prob, pd.DataFrame(new_rows)],
                           ignore_index=True)

filtered = df_vk_prob[df_vk_prob['KO id'].isin(ko_set)]
cols = ['KO id','E_value_trusted','E_values_MAG','Rk','Vk',
        'alphak','q_half','gamma_k','KO_freq','sigma','Dk']
filtered[cols].to_excel(OUTPUT_EXCEL, index=False)
print(f"KO table written to {OUTPUT_EXCEL}")

filtered[cols]

"""### Path probabilities"""

def compute_path_probability(path, dk_dict):
    """Return *Dk over all KO‑nodes in the path."""
    prob = np.float128(1.0)
    for node in path:
        if '_' in node:
            koid = node.split('_', 1)[0]
            prob *= np.float128(dk_dict.get(koid, 1.0))
    return prob

def score_path(path, dk_dict, *, alpha=0.6):
    raw_p = compute_path_probability(path, dk_dict)      # 0 < raw_p ≤ 1
    L = sum(1 for n in path if 'K' in n)                 # # of KO nodes

    if raw_p == 0:                                       # defensive
        log_p        = -np.inf
        avg_log_p    = -np.inf
        lenpen_score = -np.inf
    else:
        log_p        = float(np.log(raw_p))              # natural‑log domain
        avg_log_p    = log_p / L                         # geometric‑mean normalisation
        lenpen_score = log_p / (L ** alpha)              # Google‑style length penalty

    return {
        "L": L,
        "raw_prob"       : float(raw_p),                 # still handy to keep
        "log_prob"       : log_p,                        # Σ log pᵢ
        "avg_log_prob"   : avg_log_p,                    # (Σ log pᵢ)/L
        "geo_mean_prob"  : exp(avg_log_p),               # e^{AvgLogP}
        "lenpen_score"   : lenpen_score                  # Σ log pᵢ / L^α
    }

paths_path = f"{MODULE_JSON_DIR}/module_{module_id}_paths.json"
print("Loading:", paths_path)
with open(paths_path) as f:
    paths_dict = json.load(f)


rows_og = []
for key, comma_str in sorted(paths_dict.items(), key=lambda x: int(x[0])):
    path_nodes = [n.strip() for n in comma_str.split(',')]
    sc         = score_path(path_nodes, dk_dict)
    rows_og.append({
        "path_id":      int(key),
        "path":         " -> ".join(path_nodes),
        "raw_prob":     sc["raw_prob"],
        "avg_log_prob": sc["avg_log_prob"],
        "geo_mean_prob":sc["geo_mean_prob"],
        "lenpen_score": sc["lenpen_score"],
    })

df_path_before_diffusion = (
    pd.DataFrame(rows_og)
      .set_index("path_id")
      .sort_index()
)

'''
for key, comma_str in sorted(paths_dict.items(), key=lambda x:int(x[0])):
    path = [n.strip() for n in comma_str.split(',')]
    #print(path)
    sc = score_path(path, dk_dict)
    print(f"Path {key}: {' -> '.join(path)}")
    print("  raw_prob:",    f"{sc['raw_prob']:.6f}")
    print("  avg_log_prob:", f"{sc['avg_log_prob']:.4f}")
    print("  geo_mean_prob:",f"{sc['geo_mean_prob']:.6f}")
    print("  lenpen_score:", f"{sc['lenpen_score']:.4f}")
    #print(f"Path {key}: raw={sc['raw']:.4e}, avg_log={sc['avg_log']:.4f}, lenpen={sc['lenpen']:.4f}")
'''

df_path_before_diffusion.sort_values(by="geo_mean_prob",ascending=False)

neighbors = {}

with open(neighbor_txt, 'r') as f:
    for line in f:
        line = line.strip()
        if not line or not line.startswith("K"):
            continue
        ko, idx, nbrs_str = line.split(":", 2)
        ko = ko.strip()
        entries = [e.strip() for e in nbrs_str.split(",") if e.strip()]
        nbr_list = []
        for entry in entries:
            nbr_id, weight_str = entry.split(":")
            w = float(weight_str)
            # clamp zero weights to 0.001
            if w == 0.0:
                w = 0.001
            nbr_list.append((nbr_id.strip(), w))
        neighbors[ko] = nbr_list

dk_dict = dict(zip(df_vk_prob['KO id'], df_vk_prob['Dk']))

neighbors.get('K00844',0)

alpha = 0.6
records = []
for ko in module_kos:
    old_p = dk_dict.get(ko, np.nan)
    nbrs  = neighbors.get(ko, [])
    # unpack
    nbr_ids = [u for u,_ in nbrs]
    wts     = [w for _,w in nbrs]
    dks     = [dk_dict.get(u,0.0) for u in nbr_ids]

    #neighbor trust
    print(max(wts))
    T = alpha*max(wts)
    print(T)

    # GCN
    num = sum(w*d for w,d in zip(wts,dks))
    den = sum(wts) or 1.0
    raw_gcn = (1-T)*old_p + T*(num/den)
    new_gcn = max(old_p, raw_gcn)

    records.append({
        'KO id':                 ko,
        'Dk for KO':             old_p,
        'Neighbors':             nbrs,
        'Weights':               wts,
        'Dk for each neighbor':  [f'{d:0.4f}' for d in dks],
        'Calculated GCN': raw_gcn,
        'Final GCN':             new_gcn
    })

df_gcn = pd.DataFrame(records)

df_gcn

df_gcn.to_excel("gcn_all_n.xlsx")

"""### GCN path probabilities"""

gcn_dict = dict(zip(df_gcn['KO id'], df_gcn['Final GCN']))

rows = []
for key in sorted(paths_dict, key=lambda x: int(x)):
    path_nodes = [n.strip() for n in paths_dict[key].split(',')]
    sc         = score_path(path_nodes, gcn_dict)          # returns dict of scores
    rows.append({
        "path_id":      int(key),
        "path":         " -> ".join(path_nodes),
        "raw_prob":     sc["raw_prob"],
        "avg_log_prob": sc["avg_log_prob"],
        "geo_mean_prob":sc["geo_mean_prob"],
        "lenpen_score": sc["lenpen_score"],
    })

df_paths = (
    pd.DataFrame(rows)
      .set_index("path_id")
      .sort_index()
)

df_paths.sort_values(by="geo_mean_prob",ascending=False).to_excel("paths_post_gcn.xlsx")

df_paths.to_excel("paths_post_gcn.xlsx")

for key in sorted(paths_dict, key=lambda x: int(x)):
    path = [n.strip() for n in paths_dict[key].split(',')]
    sc   = score_path(path, gcn_dict)
    print(f"Path {key}: {' -> '.join(path)}")
    print("  raw_prob:",    f"{sc['raw_prob']:.6f}")
    print("  avg_log_prob:", f"{sc['avg_log_prob']:.4f}")
    print("  geo_mean_prob:",f"{sc['geo_mean_prob']:.6f}")
    print("  lenpen_score:", f"{sc['lenpen_score']:.4f}")

import os
import glob
import json
import pandas as pd
import numpy as np
from math import log, exp

def compute_path_probability(path, dk_map):
    p = np.float128(1.0)
    for node in path:
        if '_' in node:
            ko = node.split('_',1)[0]
            p *= np.float128(dk_map.get(ko, 1.0))
    return p

def score_path(path, dk_map, *, alpha=0.6):
    raw_p = compute_path_probability(path, dk_map)
    L = sum(1 for n in path if 'K' in n)
    if raw_p == 0:
        return {"raw_prob": 0.0}
    log_p = float(np.log(raw_p))
    avg_log = log_p / L
    geo_mean = exp(avg_log)
    lenpen = log_p / (L ** alpha)
    return {
        "raw_prob": raw_p,
        "avg_log_prob": avg_log,
        "geo_mean_prob": geo_mean,
        "lenpen_score": lenpen
    }

# (2) Loop over every module_*_paths.json in KEGG_Graphs_Generated folder
paths_files = glob.glob(os.path.join(MODULE_JSON_DIR, "module_*_paths.json"))

all_rows = []

for paths_fp in paths_files:
    # extract module_id, e.g. "M00010"
    module_id = os.path.basename(paths_fp).split("_")[1]

    # load paths
    with open(paths_fp) as f:
        paths_dict = json.load(f)

    # load module nodes to find which KOs belong to it
    nodes_fp = os.path.join(MODULE_JSON_DIR, f"module_{module_id}_nodes.json")
    with open(nodes_fp) as f:
        module_nodes = json.load(f)
    module_kos = sorted({n.split("_",1)[0]
                         for n in module_nodes
                         if n.startswith("K")})

    # GCN‐updated Dk just for this module’s KOs
    alpha = 0.6
    gcn_map = {}
    for ko in module_kos:
        old_p = dk_dict.get(ko, np.nan)
        nbrs  = neighbors.get(ko, [])
        wts   = [w for _,w in nbrs]
        dks   = [dk_dict.get(u, 0.0) for u,_ in nbrs]

        T = alpha * (max(wts) if wts else 0.0)
        num = sum(w*d for w,d in zip(wts, dks))
        den = sum(wts) or 1.0
        raw_gcn = (1 - T)*old_p + T*(num/den)
        gcn_map[ko] = max(old_p, raw_gcn)

    #for each path, score before and after GCN
    for key, comma_str in sorted(paths_dict.items(), key=lambda x: int(x[0])):
        path_nodes = [n.strip() for n in comma_str.split(',')]
        sb = score_path(path_nodes, dk_dict)
        sa = score_path(path_nodes, gcn_map)

        all_rows.append({
            "module_id":           module_id,
            "path_id":             int(key),
            "path":                " -> ".join(path_nodes),
            "raw_prob_before_gcn": sb["raw_prob"],
            "raw_prob_after_gcn":  sa["raw_prob"]
        })


df_all = pd.DataFrame(all_rows)

# e.g. write to Excel or CSV:
df_all.to_excel("all_modules_paths_gcn_comparison.xlsx", index=False)

print(df_all.head())

"""## ANALYSIS POST GCN"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import wasserstein_distance, entropy

def load_and_clean(path):
    df = pd.read_excel(path) if path.lower().endswith(('.xls', '.xlsx')) else pd.read_csv(path)

    df['before'] = pd.to_numeric(df['raw_prob_before_gcn'], errors='coerce')
    df['after']  = pd.to_numeric(df['raw_prob_after_gcn'],  errors='coerce')
    return df.dropna(subset=['before','after']).reset_index(drop=True)

df = load_and_clean("all_modules_paths_gcn_comparison.xlsx")

#Prepare distibutions

def prepare_pmf(arr, eps=1e-12):
    a = np.nan_to_num(arr.astype(float), nan=0.0, posinf=0.0, neginf=0.0)
    a += eps
    total = a.sum()
    if total <= 0 or not np.isfinite(total):
        raise ValueError(f"Invalid sum {total}")
    return a / total

p = prepare_pmf(df['before'].values)
q = prepare_pmf(df['after'].values)

#Global metrics

w1 = wasserstein_distance(np.arange(len(p)), np.arange(len(q)), u_weights=p, v_weights=q)
kl = entropy(p, q)

print(f"Wasserstein-1: {w1:.6f}")
print(f"KL(p‖q):       {kl:.6f}")

def plot_distributions(p, q):
    # a) Sorted PMF
    idx = np.argsort(p)
    plt.figure()
    plt.plot(p[idx], label='Before GCN')
    plt.plot(q[idx], label='After  GCN')
    plt.xlabel("Sorted path index")
    plt.ylabel("Probability")
    plt.title("Normalized Path-Probability Distributions")
    plt.legend()
    plt.tight_layout()

    # b) –log10 PMF
    plt.figure()
    lp, lq = -np.log10(p), -np.log10(q)
    bins = np.linspace(min(lp.min(), lq.min()), max(lp.max(), lq.max()), 50)
    plt.hist(lp, bins=bins, density=True, alpha=0.6, label='Before GCN')
    plt.hist(lq, bins=bins, density=True, alpha=0.6, label='After  GCN')
    plt.xlabel("-log10(Normalized Probability)")
    plt.ylabel("Density")
    plt.title("Distribution of –log10 Path Probabilities")
    plt.legend()
    plt.tight_layout()

plot_distributions(p, q)
plt.show()

def best_paths(df, col):
    idx = df.groupby('module_id')[col].idxmax()
    return df.loc[idx, ['module_id', 'path_id', 'path', col]].set_index('module_id')

best_before = best_paths(df, 'before')
best_after  = best_paths(df, 'after')

best = best_before.join(best_after, lsuffix='_b', rsuffix='_a')
best['changed'] = best['path_id_b'] != best['path_id_a']

print(f"\nModules changed best path: {best['changed'].sum()} / {len(best)}")

def count_node_changes(pb, pa):
    b = [n.strip() for n in pb.split('->')]
    a = [n.strip() for n in pa.split('->')]
    return len(set(b) ^ set(a))

best['node_changes'] = best.apply(lambda r: count_node_changes(r['path_b'], r['path_a']), axis=1)

print("\nNode changes per module:")
print(best['node_changes'].describe())

# Histogram of node-changes
plt.figure()
plt.hist(best['node_changes'], bins=range(int(best['node_changes'].max())+2), align='left')
plt.xlabel("Number of node changes")
plt.ylabel("Count of modules")
plt.title("Distribution of Node Changes in Best Paths")
plt.tight_layout()
plt.show()

#Frequency by positions

diffs = []
lengths = []
for _, row in best.iterrows():
    b_nodes = row['path_b'].split('->')
    a_nodes = row['path_a'].split('->')
    maxlen = max(len(b_nodes), len(a_nodes))
    diffs.append([(i < len(b_nodes) and i < len(a_nodes) and b_nodes[i]!=a_nodes[i])
                  or (i >= len(b_nodes) or i >= len(a_nodes))
                  for i in range(maxlen)])
    lengths.append(maxlen)

# Create DataFrame
diff_df = pd.DataFrame(diffs).fillna(False)
M_i = pd.Series((np.array(lengths) > diff_df.columns.values[:,None]).sum(axis=1),
                index=diff_df.columns)
C_i = diff_df.sum(axis=0)
F_i = C_i / M_i

plt.figure()
plt.plot(F_i.index, F_i.values, marker='o')
plt.xlabel("Path position (0 = first node)")
plt.ylabel("Fraction of modules with change")
plt.title("Normalized Frequency of Node Changes by Path Position")
plt.grid(True)
plt.tight_layout()
plt.show()



